-----------------------------------------------
原始日志文件

-----------------------------------------------
预处理后的文件：DataPreprocessing
log+'\t'+logName1+'\t'+time1+','+time2+','+...+','+timen+'\t'+logName1+'\t'+time1+','+time2+','+...+','+timem+....
总结就是：
日志内容+index{文件名1[时间1，时间2],文件名2[时间1，时间2，...]，...，文件名n[时间1，时间2，...]}
例：
809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT] helloworld	log1	2016-06-16 10:45:52,2016-06-16 10:46:52	log2	2016-07-16 10:45:52,2016-08-16 10:46:52
864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-728444363-127.0.1.1-1466045010214:blk_1073741825_1001 src: /127.0.0.1:37562 dest: /127.0.0.1:50010	log1	2016-06-16 10:49:06

-----------------------------------------------
Dict文件:LogToMeta


-----------------------------------------------
最终的日志文件：MetaFileSplit
log_meta+'\t'+logName1+'\t'+time1+','+time2+','+...+','+timen+'\t'+logName1+'\t'+time1+','+time2+','+...+','+timem+....
总结就是：
日志代记Meta+index{文件名1[时间1，时间2],文件名2[时间1，时间2，...]，...，文件名n[时间1，时间2，...]}
例：
0	log1	2016-06-16 10:45:53
1	log1	2016-06-16 10:45:53
10	log1	2016-06-16 10:45:54
100	log1	2016-06-16 10:45:52,2016-06-16 10:46:52
11	log1	2016-06-16 10:45:54